{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNEKHeINFb80bx3n88Akxji",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RuwanPradeep0/Sinhala-Spell-and-grammer-Checker/blob/main/Grammer_checker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch sinling\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3udR5I_c93tK",
        "outputId": "4847f701-6f3a-4ad0-b32a-5e76d98a6095"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: sinling in /usr/local/lib/python3.10/dist-packages (0.3.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (from sinling) (2.14.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sinling) (1.4.2)\n",
            "Requirement already satisfied: pygtrie in /usr/local/lib/python3.10/dist-packages (from sinling) (2.5.0)\n",
            "Requirement already satisfied: sklearn-crfsuite in /usr/local/lib/python3.10/dist-packages (from sinling) (0.5.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sinling) (3.9.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sinling) (8.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n",
            "Requirement already satisfied: python-crfsuite>=0.9.7 in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite->sinling) (0.9.11)\n",
            "Requirement already satisfied: scikit-learn>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite->sinling) (1.6.0)\n",
            "Requirement already satisfied: tabulate>=0.4.2 in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite->sinling) (0.9.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.0->sklearn-crfsuite->sinling) (1.13.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.0->sklearn-crfsuite->sinling) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import Libraries**"
      ],
      "metadata": {
        "id": "MHN9VNq_98zO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import torch\n",
        "from sinling import SinhalaTokenizer, POSTagger, SinhalaStemmer\n"
      ],
      "metadata": {
        "id": "nwzhRXR399sO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from sinling import SinhalaTokenizer, POSTagger\n",
        "import torch\n",
        "\n",
        "# Load mBERT (Multilingual BERT) model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased')\n",
        "\n",
        "# Initialize Sinling Tokenizer and POS Tagger\n",
        "sinling_tokenizer = SinhalaTokenizer()\n",
        "sinling_tagger = POSTagger()\n",
        "\n",
        "def grammar_check_with_bert(sentence):\n",
        "    # Tokenize the sentence using Sinling\n",
        "    tokens = sinling_tokenizer.tokenize(sentence)\n",
        "    pos_tags = sinling_tagger.predict([tokens])\n",
        "\n",
        "    # Print the tokenized sentence and POS tags for reference\n",
        "    print(f\"Tokens: {tokens}\")\n",
        "    print(f\"POS Tags: {pos_tags}\")\n",
        "\n",
        "    # Encode the sentence using the BERT tokenizer\n",
        "    inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "\n",
        "    # Perform inference with BERT model\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # The model's output logits (classification scores)\n",
        "    logits = outputs.logits\n",
        "    print(f\"Logits: {logits}\")\n",
        "\n",
        "    # Check if the logits correspond to an error (e.g., using a custom threshold or classification output)\n",
        "    # For simplicity, let's assume that if the logits show a negative score, it's an error\n",
        "    if logits[0][0] < 0:\n",
        "        return \"Grammar error detected!\"\n",
        "    else:\n",
        "        return \"Grammar seems correct.\"\n",
        "\n",
        "# Test with a sample sentence\n",
        "sentence = \"ඔහු ඉස්සර කුටියකට ගියේය.\"\n",
        "result = grammar_check_with_bert(sentence)\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAqGu1ad_0ZW",
        "outputId": "ca83c14a-04cf-4a06-eace-d045da961845"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['ඔහු', 'ඉස්සර', 'කුටියකට', 'ගියේය', '.']\n",
            "POS Tags: [[('ඔහු', 'PRP'), ('ඉස්සර', 'NNC'), ('කුටියකට', 'NNC'), ('ගියේය', 'VFM'), ('.', 'FS')]]\n",
            "Logits: tensor([[-0.0520,  0.1570]])\n",
            "Grammar error detected!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to check grammar using BERT for multiple sentences\n",
        "sentences = [\n",
        "    \"ඔහු ඉස්සර කුටියකට ගියේය.\",  # Example 1\n",
        "    \"මම පොතක් කියවමි.\",  # Example 2\n",
        "    \"අපි කෑම කන්න ගෙදර යන්නෙමු.\",  # Example 3\n",
        "    \"මට මිතුරන් කතා කිරීම අයුක්තයි.\",  # Example 4\n",
        "    \"ඔබේ සහාය අපට අවශ්‍යයි.\"  # Example 5\n",
        "]\n",
        "\n",
        "# Iterate over the sentences and check grammar\n",
        "for sentence in sentences:\n",
        "    result = grammar_check_with_bert(sentence)\n",
        "    print(f\"Sentence: {sentence}\")\n",
        "    print(f\"Grammar Check Result: {result}\")\n",
        "    print(\"-\" * 50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WY33l_WI__gX",
        "outputId": "7fa8a67c-f8fe-4ffc-d728-08f16305f8a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['ඔහු', 'ඉස්සර', 'කුටියකට', 'ගියේය', '.']\n",
            "POS Tags: [[('ඔහු', 'PRP'), ('ඉස්සර', 'NNC'), ('කුටියකට', 'NNC'), ('ගියේය', 'VFM'), ('.', 'FS')]]\n",
            "Logits: tensor([[-0.0520,  0.1570]])\n",
            "Sentence: ඔහු ඉස්සර කුටියකට ගියේය.\n",
            "Grammar Check Result: Grammar error detected!\n",
            "--------------------------------------------------\n",
            "Tokens: ['මම', 'පොතක්', 'කියවමි', '.']\n",
            "POS Tags: [[('මම', 'PRP'), ('පොතක්', 'NNC'), ('කියවමි', 'VFM'), ('.', 'FS')]]\n",
            "Logits: tensor([[-0.0774,  0.1802]])\n",
            "Sentence: මම පොතක් කියවමි.\n",
            "Grammar Check Result: Grammar error detected!\n",
            "--------------------------------------------------\n",
            "Tokens: ['අපි', 'කෑම', 'කන්න', 'ගෙදර', 'යන්නෙමු', '.']\n",
            "POS Tags: [[('අපි', 'PRP'), ('කෑම', 'NNC'), ('කන්න', 'NNC'), ('ගෙදර', 'NNC'), ('යන්නෙමු', 'VFM'), ('.', 'FS')]]\n",
            "Logits: tensor([[-0.0296,  0.1583]])\n",
            "Sentence: අපි කෑම කන්න ගෙදර යන්නෙමු.\n",
            "Grammar Check Result: Grammar error detected!\n",
            "--------------------------------------------------\n",
            "Tokens: ['මට', 'මිතුරන්', 'කතා', 'කිරීම', 'අයුක්තයි', '.']\n",
            "POS Tags: [[('මට', 'PRP'), ('මිතුරන්', 'NNC'), ('කතා', 'NCV'), ('කිරීම', 'VNN'), ('අයුක්තයි', 'VFM'), ('.', 'FS')]]\n",
            "Logits: tensor([[-0.0296,  0.1583]])\n",
            "Sentence: මට මිතුරන් කතා කිරීම අයුක්තයි.\n",
            "Grammar Check Result: Grammar error detected!\n",
            "--------------------------------------------------\n",
            "Tokens: ['ඔබේ', 'සහාය', 'අපට', 'අවශ්\\u200dයයි', '.']\n",
            "POS Tags: [[('ඔබේ', 'PRP'), ('සහාය', 'NNC'), ('අපට', 'PRP'), ('අවශ්\\u200dයයි', 'NNC'), ('.', 'FS')]]\n",
            "Logits: tensor([[-0.0520,  0.1570]])\n",
            "Sentence: ඔබේ සහාය අපට අවශ්‍යයි.\n",
            "Grammar Check Result: Grammar error detected!\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}